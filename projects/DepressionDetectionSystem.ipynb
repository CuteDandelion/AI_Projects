{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yx49KON1yjTD"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwiIQ5AbzRCy"
   },
   "source": [
    "\n",
    "By Ipshita Singh (3121217) & Justin Chin Chung Shin (3120632)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Depression is a widespread mental health disorder affecting millions globally. Symptoms of depression and its severity can vary from person to person. **Common symptoms of depression include** :\n",
    "\n",
    "**Emotional symptoms**\n",
    "\n",
    "*   Persistent Sadness, emptiness, or hopelessness\n",
    "*   Feeling irritable or restless\n",
    "*   Loss of interest or pleasure in activities once enjoyed\n",
    "*   Feelings of guilt, worthlessness, or helplessness\n",
    "*   Low self-esteem\n",
    "*   Difficulty concentrating, remembering, or making decisions\n",
    "\n",
    "**Physical symptoms**\n",
    "\n",
    "*   Changes in appetite or weight (either increased or decreased)\n",
    "*   Sleep disturbances (insomnia or sleeping too much)\n",
    "*   Fatigue or lack of energy\n",
    "*   Aches or pains, headaches, or digestive problems\n",
    "*   Reduced sex drive\n",
    "\n",
    "**Behavioral symptoms**\n",
    "\n",
    "*   Social withdrawal\n",
    "*   Neglect of personal hygiene\n",
    "*   Difficulty performing at work or school\n",
    "*   Increased use of alcohol or drugs\n",
    "*   Thoughts of self-harm or suicide\n",
    "\n",
    "Thus, early detection and intervention are critical for managing depression effectively.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpBRpSVL344j"
   },
   "source": [
    "Words, Languages, and expression can tell a lot about someone. In fact, the words used , how are the words being used , how frequent the words are being used and keywords can be used to detect sentiment or feelings especially in social media where everyone feels safe and comfortable sharing posts or comments about their experience due to anonymosity. With everything being said, by leveraging NLP (Natural Language Processing) , ML (Machine Learning) ,DL (Deep Learning) and comments from social media, depression sentiments can be predicted and detected.\n",
    "\n",
    "\n",
    "**Note that Sina Weibo (Chinese Social Media) comments are being used for the proof of concept to train the models **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-0uV23mQeg2"
   },
   "source": [
    "# The Evolution Of Sentimental Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztLSspruQv8d"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Back in the early 2000s**\n",
    "\n",
    "*   The birth of sentimental analysis as a subfield of NLP .\n",
    "\n",
    "**During the year 2003**\n",
    "\n",
    "*   sentimental analysis was also termed as opinion mining . At the time, opinion mining was achieved by determining the polarity of textual data, basically classifying text as positive , negative or neutral whereby a lexicon or a list of predefined keywords were used to predict or categorize sentiment. However, this approach had its limitation as rule based detection can only work within the textual data that comply with the set rules so accuracy can greatly vary if there were outliers that do not comply due to context, sarcasm, nuance, and language.\n",
    "\n",
    "**The Modern Day**\n",
    "\n",
    "*   The advancement of NLP and machine learning lead to more sophisticated sentimental analysis algorithms as the utilization of machine learning model enable the capability to detect and recognize sentiment pattern in textual data. Since then, the popularity of sentimental analysis began to rise and more companies taking initiatives to utilize sentimental analysis for business needs. For example : monitoring customer feedback, gauge public opinion , predicting financial fraud and more .\n",
    "\n",
    "\n",
    "**The Next Step For Sentimental Analysis**\n",
    "\n",
    "\n",
    "*   Emotion detection is clearly the next step for sentimental analysis as basic sentimental analysis only involve classifying text within the range of positive, neutral and negative while emotion detection will go beyond sentiment polarity , capable of detecting & recognizing emotions and emotional undertones or expressions such as sarcasm, satire, humour, joy, surprise, fear and disgust.\n",
    "\n",
    "\n",
    "*  By leveraging the capability of emotion detection,various industry can benefit significantly . Examples of Application include :     \n",
    "\n",
    "\n",
    "1.   Financial Sentiment Calibration\n",
    "2.   Customer Feedback Optimisation\n",
    "3.   Mental Health Assistant\n",
    "4.   Political Analysis\n",
    "5.   Marketing & Advertising\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbmFvPtbLbQD"
   },
   "source": [
    "# Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taTZTj6uYUSX"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "*   The objective is to identify the best sentimental analysis model in terms of detecting and predicting depression sentiment in chinese textual data or user comments from Sina Weibo, also in terms of stability.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrKVuYOzngZl"
   },
   "source": [
    "# Rule Of The Chinese Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVpE8A0Dn-N_"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Key Characteristics of Chinese Grammar**\n",
    "\n",
    "\n",
    "1.   **No Verb Conjugations**: Verbs maintain their form regardless of person, number, or tense.\n",
    "2.   **No Articles**: Chinese doesn't use articles like \"the\" or \"a.\"\n",
    "3. **No Plural Forms**: Plurality is often implied or indicated by context or quantity words.\n",
    "4. **Subject-Verb-Object (SVO) Order**: This is the basic sentence structure.\n",
    "5. **Particles**: These small words help indicate tense, aspect, mood, and other nuances.\n",
    "6. **Word Order is Crucial**: The position of words in a sentence determines meaning.\n",
    "\n",
    "\n",
    "**Basic Sentence Structure**\n",
    "A simple Chinese sentence follows the Subject-Verb-Object (SVO) order:\n",
    "\n",
    "\n",
    "\n",
    "*   **Subject**: The person or thing performing the action.\n",
    "*   **Verb**: The action being performed.\n",
    "*   **Object**: The person or thing receiving the action.\n",
    "\n",
    "\n",
    "**Example:**\n",
    "我 吃 苹果 (Wǒ chī píngguǒ) - I eat apple.\n",
    "\n",
    "**Other Important Points To Note**\n",
    "\n",
    "*   **Tone**: Chinese is a tonal language, meaning the pitch of your voice can change the meaning of a word.\n",
    "*   **Characters**: Chinese uses characters instead of an alphabet, and each character represents a word or concept.\n",
    "*   **Idioms**: Chinese is rich in idioms, which can be challenging for learners.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed-QJpz_y4pi"
   },
   "source": [
    "# Proof Of Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-2MUyO0oAQf"
   },
   "source": [
    "**There are 4 sections to the proof of concepts :**   \n",
    "\n",
    "\n",
    "*   Data Preprocessing - Preparing of chinese textual data (comments) in order to ensure readiness for model training.\n",
    "\n",
    "*   Machine Learning (Training & Evaluating) - Training ML models (linear regression, random forest , and a custom ensemble-model through models stacking classification) while also evaluating them.\n",
    "\n",
    "*   Deep Learning (Training & Evaluating) - Training torch Neural networks (BiLSTM RNN model) and transformer (BERT) , also evaluating them.\n",
    "\n",
    "*   Prototype - Based on the result of the evaluations, demonstrate the pipeline from raw comments to prediction with the best model (in terms of stability and accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdW83hKKZC8-"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zW8Jr_sfnYph"
   },
   "source": [
    "In order to preprocess data, we require data . To be more specific, the social media comments from Sina Weibo (chinese textual data).\n",
    "\n",
    "To achieve this, google collab platform will perform a git clone to clone the required data and lexicon onto the platform itself for ease of use, so no uploading is required.\n",
    "\n",
    "Basically, the rest is just installing required python libraries and importing them for use. Create a models directory to store saved models and reading the lexicon into a pandas dataframe (consists of context which are keywords and depression_point which is the sentiment point whereby 1 = depressed and 0 = healthy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 962
    },
    "id": "VnpDCJCYOZnh",
    "outputId": "2f19b16f-2317-47aa-e0fe-1b246229d59d"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/CuteDandelion/DepressionDetectionDataset_Chinese.git\n",
    "!pip install emoji\n",
    "!pip install google_trans_new\n",
    "!pip install thulac\n",
    "!pip install Keras-Preprocessing\n",
    "\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "import os\n",
    "\n",
    "directory_name = \"models\"\n",
    "os.makedirs(directory_name, exist_ok=True)\n",
    "\n",
    "#The lexicon include keywords that are commonly assiociated with depression\n",
    "\n",
    "lexicon_df = pd.read_csv(\"/content/DepressionDetectionDataset_Chinese/depressionLexiconNew.csv\", encoding=\"utf-8\")\n",
    "lexicon_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohAvv4I4tZYB"
   },
   "source": [
    "The purpose here is basically to initialize the dataframe namely df with Tweet being the chinese comments and Depressed being the depression state (consist of 2 classes 0 being healthy and 1 being depressed) by concatinating male depression comments , female depression comments and male control comments (which consists of healthy comments).\n",
    "\n",
    "The reason female control comments is not added is because\n",
    "\n",
    "\n",
    "1.   the variety of depressed comments are being priotized due to the fact that if the model can recognize depressed comments than the others must be healthy comments.\n",
    "2.   The list of healthy comments for female audience is huge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OpKQZ5QvOfHD",
    "outputId": "bbd1fa56-85b2-45e2-ab5b-2bb1d23e895f"
   },
   "outputs": [],
   "source": [
    "#Initializing starting dataframe with tweet being the comments text\n",
    "# and depressed being the state of depression\n",
    "\n",
    "control_file = '/content/DepressionDetectionDataset_Chinese/control_comments_male.txt'\n",
    "depressed_file_male = '/content/DepressionDetectionDataset_Chinese/depressed_comments_male.txt'\n",
    "depressed_file_female = '/content/DepressionDetectionDataset_Chinese/depressed_comments_female_origin.txt'\n",
    "\n",
    "with open(depressed_file_female, \"r\", encoding=\"utf-8\") as dfe:\n",
    "    dfe_lines = dfe.readlines()\n",
    "    dfe_lines = [x.replace(\"\\n\",\"\") for x in dfe_lines]\n",
    "\n",
    "with open(depressed_file_male, \"r\", encoding=\"utf-8\") as d:\n",
    "    d_lines = d.readlines()\n",
    "    d_lines = [x.replace(\"\\n\",\"\") for x in d_lines]\n",
    "\n",
    "with open(control_file, \"r\", encoding=\"utf-8\") as c:\n",
    "    c_lines = c.readlines()\n",
    "    c_lines = [x.replace(\"\\n\",\"\") for x in c_lines]\n",
    "\n",
    "depressed_male = pd.DataFrame(d_lines,columns=[\"Tweet\"])\n",
    "depressed_male[\"Depressed\"] = 1\n",
    "depressed_female = pd.DataFrame(dfe_lines,columns=[\"Tweet\"])\n",
    "depressed_female[\"Depressed\"] = 1\n",
    "control = pd.DataFrame(c_lines,columns=[\"Tweet\"])\n",
    "control[\"Depressed\"] = 0\n",
    "\n",
    "df = pd.concat([depressed_male, depressed_female, control], ignore_index=True)\n",
    "df = df[df['Tweet'] != '']\n",
    "df = df[df['Tweet'] != '转发微博']\n",
    "df = df.sample(frac = 1)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "aGEVjU-L_ZjD",
    "outputId": "caa3b2d1-955b-4364-94ad-604a27a6bf23"
   },
   "outputs": [],
   "source": [
    "print(\"Dataset size:\",df.shape)\n",
    "df.groupby(\"Depressed\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDWExLzZv-YX"
   },
   "source": [
    "Basically, the code below describe the\n",
    "\n",
    "*   removal of \" \" or empty comments from dataframe.\n",
    "\n",
    "*   utilization of emoji library to detect emojicons in comments or tweets and translate each emoji to their respective chinese representations.\n",
    "\n",
    "*   the use of regular expressions to substitute all non chinese symbols for each comment or tweet in the dataframe into \"\" (basically removing non chinese symbols)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c75MJEq-iook",
    "outputId": "2abdbb59-0f22-4662-d9f6-6e265df00a7d"
   },
   "outputs": [],
   "source": [
    "from google_trans_new import google_translator as Translator\n",
    "from joblib import Parallel, delayed\n",
    "import thulac #https://github.com/thunlp/THULAC-Python\n",
    "import emoji\n",
    "\n",
    "translator = Translator()\n",
    "chinese_tokenizer = thulac.thulac(seg_only=True)\n",
    "\n",
    "def remove_noice(tweet):\n",
    "     tweet = re.sub(r\"[^\\u4e00-\\u9fff]\",\"\", tweet.strip())\n",
    "     return tweet.replace(\" \",\"\")\n",
    "\n",
    "def replace_emoji(tweet):\n",
    "  emoji_list = emoji.distinct_emoji_list(tweet)\n",
    "\n",
    "  def translate_emoji(emoji_icon):\n",
    "    first_token = next(emoji.analyze(emoji_icon))\n",
    "    return first_token.value.data['zh'] if first_token else \"\"\n",
    "\n",
    "  for emoji_icon in emoji_list:\n",
    "    tweet = tweet.replace(emoji_icon, translate_emoji(emoji_icon))\n",
    "  return tweet\n",
    "\n",
    "\n",
    "\n",
    "df = df[df['Tweet'] != \" \"]\n",
    "df[\"emojilessTweet\"] = df[\"Tweet\"].apply(lambda x: replace_emoji(x))\n",
    "df[\"noiselessTweet\"] = df[\"emojilessTweet\"].apply(lambda x: remove_noice(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TMWZTcStqCgR",
    "outputId": "7b6d6de7-3dd5-4836-9ab8-7a1ad7765464"
   },
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-TC4IgGzIEN"
   },
   "source": [
    "Filter the lexicon so that it consists only keywords assosiate with depression and convert the result into a list.\n",
    "\n",
    "generate a dataframe namely filtered_depressed_df by concatenation of targeted (being comments which contains the depressed keywords provided by the lexicon) and not targeted (being comments which does not contain the keywords provided by lexicon but still categorized as depressed comments)\n",
    "\n",
    "The reason being\n",
    "\n",
    "\n",
    "1.   To manage and provide control over the size of datasets to reduce the time spent and resource used to remove stopwords and tokenization of chinese characters as it is different from english .\n",
    "2.   An attempt to reduce or minimize overfitting and bias so that the training set and testing set does not contain only comments assosiated with the lexicon but also of different depressed comment varieties.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "Lfc1Zoj4m_gU",
    "outputId": "69eee225-bb5b-4a4d-f327-be2229d115b2"
   },
   "outputs": [],
   "source": [
    "depressed_lexicon_keywords = lexicon_df[\"context\"][lexicon_df[\"depression_point\"]==1].tolist()\n",
    "pattern = re.compile('|'.join(depressed_lexicon_keywords))\n",
    "\n",
    "filtered_depressed_df = df[df['Depressed'] == 1]\n",
    "filtered_depressed_df_targeted = filtered_depressed_df[filtered_depressed_df[\"noiselessTweet\"].astype(str).apply(lambda x: pattern.search(x) is not None)].sample(n=500, random_state=42)\n",
    "filtered_depressed_df_notTargeted = filtered_depressed_df[filtered_depressed_df[\"noiselessTweet\"].astype(str).apply(lambda x: pattern.search(x) is None)].sample(n=500, random_state=42)\n",
    "filtered_depressed_df = pd.concat([filtered_depressed_df_targeted, filtered_depressed_df_notTargeted], ignore_index=True)\n",
    "filtered_depressed_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t92EpW_Z1dbk"
   },
   "source": [
    "To generate a dataframe containing healthy comments which have the same number of records as depressed comments to avoid class imbalance and ensuring fair class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "CtDFxmytw7bN",
    "outputId": "810c303c-69b0-42ee-ec10-46de868e2096"
   },
   "outputs": [],
   "source": [
    "healthy_lexicon_keywords = lexicon_df[\"context\"][lexicon_df[\"depression_point\"]==0].tolist()\n",
    "pattern = re.compile('|'.join(healthy_lexicon_keywords))\n",
    "\n",
    "filtered_healthy_df = df[df['Depressed'] == 0]\n",
    "filtered_healthy_df = filtered_healthy_df.sample(n=len(filtered_depressed_df), random_state=42)\n",
    "#filtered_healthy_df = filtered_healthy_df[filtered_healthy_df[\"noiselessTweet\"].astype(str).apply(lambda x: pattern.search(x) is not None)]\n",
    "filtered_healthy_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COWoRlW-2Lmw"
   },
   "source": [
    "generate a sample dataframe out of filtered_depressed_df and filtered_healthy_df (both mentioned above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "id": "kx4tkS18xBOT",
    "outputId": "03b4ee32-e3ed-464d-8fc9-8312dc52d676"
   },
   "outputs": [],
   "source": [
    "### Sample dataframe after filtering with lexicon\n",
    "sampling_df = pd.concat([filtered_depressed_df, filtered_healthy_df], ignore_index=True)\n",
    "sampling_df = sampling_df.sample(frac = 1)\n",
    "sampling_df.reset_index(drop=True, inplace=True)\n",
    "sampling_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "UtSYdIhKIQ5P",
    "outputId": "2999c887-89b9-42e0-8a03-3ad3551eba85"
   },
   "outputs": [],
   "source": [
    "print(\"Dataset size:\",sampling_df.shape)\n",
    "sampling_df.groupby(\"Depressed\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JxE_xEQ6U2c"
   },
   "source": [
    "we imported stopwords specifically in the chinese language and remove any words in the list of stopwords that is based on the keywords in the lexicon to prevent removing any important features from the chinese comments.\n",
    "\n",
    "Basically, the cleanTweet in sampling_df will store the tokenized comments or features after removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "kXGFgLVSx0WR",
    "outputId": "16bbf582-4eb4-4035-fce4-366b9da9ec50"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words( 'chinese' )\n",
    "stopwords = [word for word in stopwords if word not in depressed_lexicon_keywords]\n",
    "stopwords = [word for word in stopwords if word not in healthy_lexicon_keywords]\n",
    "\n",
    "def remove_stopwords(tweet):\n",
    "  tweet = chinese_tokenizer.cut(tweet, text=True)\n",
    "  features = tweet.split(\" \")\n",
    "  tweet = \" \".join([word for word in features if word not in stopwords])\n",
    "  return tweet\n",
    "\n",
    "sampling_df[\"cleanTweet\"] = sampling_df[\"noiselessTweet\"].apply(lambda x: remove_stopwords(x))\n",
    "sampling_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b_xTiKA-EWu"
   },
   "source": [
    "## Machine Learning (Training & Evaluating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMRjZpIJ-UnA"
   },
   "source": [
    "Before using the comments from data preprocessing for inputs for the models. We\n",
    "\n",
    "\n",
    "\n",
    "*   compute the sentiment score by aggregating the number of keywords appearing in each comment or tweet .\n",
    "\n",
    "*   initialize TFIDF (Term Frequency Inverse Document Frequency) vectorizer and begin vectorize the comments and tweets thereby converting textual data into a list containing vectors of numbers (indicating the frequency and importance in the overall data) , in other words converting text data into numerical features to help the model to determine context .\n",
    "\n",
    "*  combine the sentiment scores and the features to perform feature engineering , in a way to add weights to comments or tweets which contains certain keyword to help improve accuracy of model.\n",
    "\n",
    "Lastly, splitting the dataset into training set and testing set (each consists of X (referring to the features) and Y (the labels))\n",
    "\n",
    "* Note that the TFIDF vectorizer will be saved after fitting / training so that it could be used later on during prototyping so that retraining is not needed since transform assign a token to a numerical value or frequency based on fit_transform which actually trains the model to assign the correct numerical value or frequency to a token in a comment or tweet.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BP7cNSst-Z-R"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score, f1_score\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "X = sampling_df['cleanTweet']\n",
    "y = sampling_df['Depressed']\n",
    "\n",
    "\n",
    "\n",
    "def get_sentiment_score(word, lexicon_df):\n",
    "    score_row = lexicon_df.loc[lexicon_df['context'] == word, 'depression_point']\n",
    "    if not score_row.empty:\n",
    "        return score_row.values[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_sentiment_score(X_sample, lexicon_df):\n",
    "    words = X_sample.split()\n",
    "    sentiment_score = sum([get_sentiment_score(word, lexicon_df) for word in words])\n",
    "    return sentiment_score\n",
    "\n",
    "sentiment_scores = X.apply(lambda X_sample: compute_sentiment_score(X_sample, lexicon_df))\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "joblib.dump(tfidf_vectorizer, os.path.join(directory_name,'tfidf_vectorizer.pkl'))\n",
    "\n",
    "# Combine TF-IDF vectors with sentiment scores\n",
    "combined_features = np.hstack((tfidf_vectors, sentiment_scores.values.reshape(-1, 1)))\n",
    "\n",
    "#splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features , y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8YUGqjI99JoH",
    "outputId": "30697530-8b0d-4913-ad99-623015f58c47"
   },
   "outputs": [],
   "source": [
    "#vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = X_train\n",
    "X_test_tfidf = X_test\n",
    "\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5oAjZjLIJjE"
   },
   "source": [
    "Initialize a random forest (RF) classifier model with n_estimators of 100 to construct 100 weak individual trees to perform addition  of predictions for better accuracy and random state of 42 for ease of reproducibility among models by introducing fixed randomness factors to reduce unpredictability during training.\n",
    "\n",
    "By calling fit , the RF model will start training with the vectorized training set for the comments or tweets and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "oLrYYks_9Jra",
    "outputId": "e90e9835-6a51-4a9c-fd40-aa6e46ae28d4"
   },
   "outputs": [],
   "source": [
    "#training the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b1uF_U4KI9t"
   },
   "source": [
    "Here, RF model will begin predicting the y-test label for the vectorized comments and tweets from the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OIG1H9V-aBE"
   },
   "outputs": [],
   "source": [
    "#predictions\n",
    "y_pred_rfc = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5G8uNaCKggb"
   },
   "source": [
    "Basically, here we evaluate the performance of RF model by evaluating 5 metrics\n",
    "\n",
    "\n",
    "\n",
    "1.   accuracy (defines the model tendency to predict correct labels for both healthy and depressed comments)\n",
    "2.   precision (the tendency for model to avoid false positives)\n",
    "3.   recall (the tendency for model to avoid false negatives)\n",
    "4.   F1-scores (precision + recall / 2 , average performance)\n",
    "4.   confusion matrix (the matrix tables of positives, false positives, negative, false negatives)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rAfNll-S9mD2",
    "outputId": "e9111e07-0c78-4927-c963-4050f6bfb915"
   },
   "outputs": [],
   "source": [
    "#evaluation of the model\n",
    "accuracy = accuracy_score(y_test, y_pred_rfc)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred_rfc))\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred_rfc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "RpIHuSNz9miJ",
    "outputId": "e6f95b5c-4eb2-436d-dfcb-bb31f95bfe9b"
   },
   "outputs": [],
   "source": [
    "conf_matrix_lr = confusion_matrix(y_test, y_pred_rfc)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_lr, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Depressed', 'Depressed'], yticklabels=['Not Depressed', 'Depressed'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDwifFHYEoxl"
   },
   "source": [
    "In regards to logistic regression, setting max_iter to 2000 here would be to limit the running algorithm to only iterate 2000 times before it stop (also termed as early stopping) , basically to save computational resources and also to reduce overfitting by reducing exposure to unnecessary noices in the features.\n",
    "\n",
    "And fit would be called to train the model and finally predict is called to make prediction based on vectorized comments/tweets in the testing set.\n",
    "\n",
    "Finally, calling classification_report() to render the evaluation report so that we could evaluate the model in terms of accuracy, precision, recall , F1-score and confusion matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NhMJs5kT-AFS",
    "outputId": "30e58eee-27bd-4624-cf90-d892ae00d244"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Logistic Regression model\n",
    "model = LogisticRegression(max_iter=2000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "#predictions and evaluation\n",
    "y_pred_lr = model.predict(X_test_tfidf)\n",
    "log_reg_report = classification_report(y_test, y_pred_lr)\n",
    "print(log_reg_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "bAQLLEbV-AHt",
    "outputId": "357b91be-06e3-4faa-9903-42b6be19b755"
   },
   "outputs": [],
   "source": [
    "conf_matrix_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_lr, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Depressed', 'Depressed'], yticklabels=['Not Depressed', 'Depressed'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWwKF9AiQ-n1"
   },
   "source": [
    "And here, we attempt to create a custom model and define a function based on an ensemble method of model stacking (an interesting method whereby the estimators of the base models will each learn different features from the training set and combining the features to form meta-features which would be then given to the final estimator which would be the meta-model as input to make the final prediction )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0eSqZ8ipA1A"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import StackingClassifier, HistGradientBoostingClassifier,AdaBoostClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "\n",
    "def generate_custom_model():\n",
    "    estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=50, random_state=42, max_depth=10)),\n",
    "    ('knn',KNeighborsClassifier(n_neighbors=7)),\n",
    "    ('svc', SVC(kernel='linear', C=1, random_state=42)),\n",
    "    ('NB', GaussianNB()),\n",
    "    ('svr', make_pipeline(StandardScaler(),LinearSVC(random_state=42)))\n",
    "    ]\n",
    "    model = StackingClassifier(\n",
    "           estimators=estimators, final_estimator=LogisticRegression(max_iter=2000)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nUfgyEGUHyw"
   },
   "source": [
    "Similar to the other RF and LR models mentioned , fit is called to train the custom model (referring to the stacking classifier ensemble model) by providing X, Y training set as input. Follow by saving the model to the models directory , loading the model to test it by also calling the predict method to make prediction based on X testing set which consists of the chinese social media comments .\n",
    "\n",
    "Finally, calling classification report and also rendering the confusion matrix to perform model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kCSb4DSAB1a",
    "outputId": "bb843d24-ae0a-4828-bf0e-d66d0a4ca295"
   },
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "from joblib import load\n",
    "\n",
    "X_train_new = X_train_tfidf\n",
    "X_test_new = X_test_tfidf\n",
    "\n",
    "custom_model = generate_custom_model()\n",
    "\n",
    "custom_model.fit(X_train_new, y_train)\n",
    "\n",
    "dump(custom_model, os.path.join(directory_name, \"stacking_classifier_model.pkl\"))\n",
    "ensemble_model = load(os.path.join(directory_name, \"stacking_classifier_model.pkl\"))\n",
    "\n",
    "y_pred_cm = ensemble_model.predict(X_test_new)\n",
    "\n",
    "custom_model_report = classification_report(y_test, y_pred_cm)\n",
    "print(custom_model_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "drHvG33eSAb0",
    "outputId": "6f0e53c5-0003-4208-b12b-ddf68d59ecaf"
   },
   "outputs": [],
   "source": [
    "conf_matrix_lr = confusion_matrix(y_test, y_pred_cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_lr, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Depressed', 'Depressed'], yticklabels=['Not Depressed', 'Depressed'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oupsCNQxViU-"
   },
   "source": [
    "Finally, all 3 models will be compared by extracting the 3 metrics (precision, recall, f1-score) from the classification reports and basically plot them using a line chart .\n",
    "\n",
    "Overall, the ensemble model seems to perform better than individual models in terms of stability or performance consistency . In other words, the 3 metrics for the stack ensemble model are typically balanced AND actually perform much better than LR and RF models over several run iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "D526Jz-aBHwj",
    "outputId": "082386ca-068e-48e5-8935-793f9f6b948e"
   },
   "outputs": [],
   "source": [
    "#extracting metrics from classification reports\n",
    "log_reg_report = classification_report(y_test, y_pred_lr, output_dict=True)\n",
    "rf_report = classification_report(y_test, y_pred_rfc, output_dict=True)\n",
    "custom_model_report = classification_report(y_test, y_pred_cm, output_dict=True)\n",
    "\n",
    "#extracting metrics\n",
    "metrics = ['precision', 'recall', 'f1-score']\n",
    "log_reg_scores = [log_reg_report['macro avg'][metric] for metric in metrics]\n",
    "rf_scores = [rf_report['macro avg'][metric] for metric in metrics]\n",
    "custom_model_scores = [custom_model_report['macro avg'][metric] for metric in metrics]\n",
    "\n",
    "#plotting the metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(metrics))\n",
    "\n",
    "plt.plot(x, log_reg_scores, label='Logistic Regression', marker='o')\n",
    "plt.plot(x, rf_scores, label='Random Forest', marker='o')\n",
    "plt.plot(x, custom_model_scores, label='Ensemble Stack Model', marker='o')\n",
    "\n",
    "plt.xticks(x, metrics)\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdmwXhiGZlUd"
   },
   "source": [
    "## Deep Learning (Training & Evaluating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xHAJ9IaNYJ0"
   },
   "source": [
    "In this section for deep learning, the focus is on training and evaluating BiLSTM (BiDirectional Long Short Term Memory) model and transformer (BERT) model.\n",
    "\n",
    "With that being said, pytorch is chosen as the main python library to build these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ky7cZcafEEPt"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Bidirectional, Dense, Dropout, Concatenate, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgbK1GC4Q0Mg"
   },
   "source": [
    "SO, similar to the training of machine learning model, we first have to vectorize the comments/tweets in order to train the deep learning models.\n",
    "\n",
    "Thus, we make a copy of sampling_df. Running a tokenizer to train on the comments and tweets in the dataframe in order to determine appearance frequencies while also initialize the vocabulary size (referring to the unique count of word detected in the tweets / comments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BDFY5l6gwfE"
   },
   "outputs": [],
   "source": [
    "sampling_df_new = sampling_df.copy()\n",
    "\n",
    "#tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sampling_df_new['cleanTweet'])\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXjQmDAOSfRt"
   },
   "source": [
    "Here, we basically define sequenced_text in sampling_df to store the vectorized or sequenced comments / tweets ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 779
    },
    "id": "SJvUn7fQXxGR",
    "outputId": "5fcb555e-021a-44a8-e6a8-1dc5d615cdc6"
   },
   "outputs": [],
   "source": [
    "#convert text to sequences\n",
    "sampling_df_new['sequenced_text'] = tokenizer.texts_to_sequences(sampling_df_new['cleanTweet'])\n",
    "sampling_df_new.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpvfFfzvS4s2"
   },
   "source": [
    "In order to train deep learning models, we made sure to insert paddings of 0s to each sequenced comments/tweets to make sure that each sequenced data have equal length or shape due to deep learning models are better at recognizing pattern in sequential data and often require that the inputs are of equal shapes.\n",
    "\n",
    "In this case, we ensure that each sequenced data are shaped at 100 in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 796
    },
    "id": "3-82gj5RYCcI",
    "outputId": "66ef3741-4742-4935-9637-8446339c8c3d"
   },
   "outputs": [],
   "source": [
    "#pad the sequences\n",
    "max_len = 100\n",
    "sampling_df_new['sequenced_text'] = pad_sequences(sampling_df_new['sequenced_text'], maxlen=max_len, padding='post').tolist()\n",
    "\n",
    "sampling_df_new.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3emwfzhVBU1"
   },
   "source": [
    "Also, similar to the approach being used in the previous section for machine learning , we compute the sentiment score for each tweets and comments in cleanTweet by aggregating the number of keywords appearance based on the depression_point of the defined lexicon.\n",
    "\n",
    "Next, we concatenate the sequenced comments/ tweets and the sentiment score to combine the features to form the input for the deep learning models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1NlguDCTbdNK",
    "outputId": "2e8decc0-0d45-4739-e7c9-9f91ada7cfe0"
   },
   "outputs": [],
   "source": [
    "def get_sentiment_score(word, lexicon_df):\n",
    "    score_row = lexicon_df.loc[lexicon_df['context'] == word, 'depression_point']\n",
    "    if not score_row.empty:\n",
    "        return score_row.values[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_sentiment_score(X_sample, lexicon_df):\n",
    "    words = X_sample.split()\n",
    "    sentiment_score = sum([get_sentiment_score(word, lexicon_df) for word in words])\n",
    "    return sentiment_score\n",
    "\n",
    "sampling_df_new['sentiment_score'] = sampling_df_new['cleanTweet'].apply(lambda X_sample: compute_sentiment_score(X_sample, lexicon_df))\n",
    "\n",
    "sequenced_text = np.array(sampling_df_new['sequenced_text'].tolist())\n",
    "sentiment_score = np.array(sampling_df_new['sentiment_score']).reshape(-1, 1)\n",
    "\n",
    "combined_features = np.hstack((sequenced_text, sentiment_score))\n",
    "\n",
    "# Add the combined features back to the DataFrame\n",
    "sampling_df_new['combined_features'] = combined_features.tolist()\n",
    "\n",
    "sampling_df_new.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZvIxMyGWspi"
   },
   "source": [
    "Define TweetDataset class to help with converting sequenced data to PyTorch tensors in order for Torch deep learning models to work with the data for training purposes.\n",
    "\n",
    "A torch tensors will consists of data , text and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RnPkL0qY4LA"
   },
   "outputs": [],
   "source": [
    "#convert data to PyTorch tensors\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.tensor(self.texts[idx], dtype=torch.long)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVXU9HjEcNie"
   },
   "source": [
    "As always, splitting the features dataset into training sets and testing sets with distribution of 80% and 20% respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSRWzaJ6ZBVp"
   },
   "outputs": [],
   "source": [
    "#split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sampling_df_new['combined_features'], sampling_df_new['Depressed'], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XxeC6KYcolk"
   },
   "source": [
    "Here, we define and intialize dataloaders to convert the datasets into batches and in this case, batch size of 32 in order to control and manage the computational resource when using the training or testing datasets for training deep learning models since it is generally resource intensive or expensive to train the entire dataset all at once due to datasets may have large amount of features or neurons ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBxTFrZyZUio"
   },
   "outputs": [],
   "source": [
    "#create DataLoader\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TweetDataset(X_train.tolist(), y_train.tolist())\n",
    "test_dataset = TweetDataset(X_test.tolist(), y_test.tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfnbWpIrej_x"
   },
   "source": [
    "In regards to the Bidirectional - LSTM model, we define here the specifications consisting of 4 layers\n",
    "\n",
    "\n",
    "\n",
    "1.   **first layer - embedding layer** (Basically converting vectors into model readable dense format.)\n",
    "2.   **second layer - bilstm layer** (Basically to detect important features in generally long sequential data - output from first layer by scanning left and right direction in parallel to cover more grounds and store the context or important feature in memory cells. Also concatination of the features. )\n",
    "3.   **third layer - linear layer** (Basically taking the features from the second layer to compute weighted sums of the inputs.)\n",
    "4.   **fourth layer - sigmoid layer** (Basically to take the output of the third layer as input and apply sigmoid function to ensure non-linearity by converting the results of the third layers to value between 0 and 1 .)\n",
    "\n",
    "So, how it works is that for each layer in the neural network, the next layer will take the output of previous layers as input during the forward pass.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DDCU5kkZnG_"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Enhanced_RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units, lstm_units, gru_units, output_size, num_layers=1):\n",
    "        super(Enhanced_RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        #self.rnn = nn.RNN(embedding_dim, rnn_units, num_layers=num_layers, batch_first=True)\n",
    "        self.bilstm = nn.LSTM(embedding_dim, lstm_units, num_layers=num_layers,\n",
    "                            bidirectional=True, batch_first=True)\n",
    "        #self.gru = nn.GRU(lstm_units * 2, gru_units, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_units * 2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #rnn_out, _ = self.rnn(x)\n",
    "        bilstm_out, _ = self.bilstm(x)\n",
    "        #gru_out, _ = self.gru(bilstm_out)\n",
    "        # Take the output from the last time step\n",
    "        bilstm_out = bilstm_out[:, -1, :]\n",
    "        out = self.fc(bilstm_out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5DdIO17kicD"
   },
   "source": [
    "After defining the model, we\n",
    "\n",
    "\n",
    "\n",
    "1.   initialize the model\n",
    "2.   intialize the criterion (also termed as loss function, the metrics to use for calculating loss)\n",
    "3.   initialize the optimizer (basically to update and adjust weights in a neural network in accordance of the loss computation between the network predicted values and actual target values )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BcAsZlXZ4eY"
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "embed_size = 128\n",
    "hidden_size = 128\n",
    "output_size = 2\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "lstm_units = 256\n",
    "rnn_units = 128\n",
    "gru_units = 256\n",
    "\n",
    "#initialize model, loss function, and optimizer\n",
    "model = Enhanced_RNN(vocab_size, embed_size, rnn_units, lstm_units, gru_units, output_size, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJ16nYkvmm2L"
   },
   "source": [
    "The train_model function is defined in order to train the biLSTM model basically repeat the iteration (based on number of epochs) of\n",
    "\n",
    "\n",
    "**Forward Pass**\n",
    "1.   predicting the labels value (pred) based on the training set (referring to the training loader)\n",
    "2.   initializing loss function\n",
    "\n",
    "**Backward Pass & Optimization**\n",
    "3.   reset the gradient to 0 to prevent accumulation of gradient to reduce the risk of model overfitting by not over-influencing the next iteration to past gradients and also to prevent the gradient becoming too huge.\n",
    "4.  calculating loss by comparing the differences of pred labels vs actual target labels and initiate backward propagation. Also storing the gradient.\n",
    "5.  optimizer will update and adjust the weights in accordance to the loss computation (referring to the gradients) from the criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7p55hwhqaYfe"
   },
   "outputs": [],
   "source": [
    "#training function\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for texts, labels in tqdm(train_loader):\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "            #forward pass\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            #backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKfnbc3hyre9"
   },
   "source": [
    "The evaluate_model function is defined to evaluate the biLSTM model by first predicting the labels of the testing set (referring to the test loader) and basically extracting the metrics (accuracy, recall, F1-score) to plot a bar chart to compare the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OZefZDcabbH"
   },
   "outputs": [],
   "source": [
    "#evaluation function\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in test_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    print(\"RNN Classification Report:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "    #visualization\n",
    "    metrics = ['Accuracy', 'Recall', 'F1-Score']\n",
    "    scores = [accuracy, recall, f1]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(metrics, scores, color=['blue', 'green', 'red'])\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title('Performance Metrics for RNN Model')\n",
    "    for i, v in enumerate(scores):\n",
    "        plt.text(i, v + 0.02, f\"{v:.2f}\", ha='center', fontweight='bold')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-o-3kk0Izdbt"
   },
   "source": [
    "Setting the device configuration to either make use of the **GPU** (which perform significantly well compare to cpu) if available else setting the device to **cpu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-oaZdESfaiPn"
   },
   "outputs": [],
   "source": [
    "#setting device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Co7AVvtGzzR6"
   },
   "source": [
    "Training the biLSTM model and save the model for later used if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KO-Nyfj0akFv",
    "outputId": "1c53f5ac-00ae-4ad4-e3c1-561b0824cdd3"
   },
   "outputs": [],
   "source": [
    "#training the model\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "import torch\n",
    "torch.save(model.state_dict(), os.path.join(directory_name, \"enhanced_bilstm_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sscw-Izx0CNz"
   },
   "source": [
    "Test the loading of biLSTM model and evaluate model performance.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "6KRTXEIFjgXp",
    "outputId": "fb022aa1-2d15-4896-a7e4-12c343a1c7a4"
   },
   "outputs": [],
   "source": [
    "bilstm_model = Enhanced_RNN(vocab_size, embed_size, rnn_units, lstm_units, gru_units, output_size, num_layers)\n",
    "bilstm_model.load_state_dict(torch.load(os.path.join(directory_name, \"enhanced_bilstm_model.pth\")))\n",
    "\n",
    "#evaluating\n",
    "evaluate_model(bilstm_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brXbGOMTo4hS"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4c6MbcBq38P"
   },
   "source": [
    "Similar to TweetDataset mentioned above, TweetDatasetBERT is basically doing the same thing except that here BERT tokenizer is being used to encode the data (also includes tokenization and padding) to form torch tensors  consisting of flatten input_ids, flatten attention_mask and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "79d00e93a421405ca3ce2ae935e6da0c",
      "9df1bd1909d84fc0a05ed5d337758f9c",
      "22a641bcb7544f0388aea5ab7771aee7",
      "7373689a86f546a0b3654169df24e425",
      "52c585ad46d64c2ba517b70aa28d0422",
      "dd32010360c1498db4f98437468c29de",
      "30efb00829844afeb2a84d216ea1d48a",
      "de8f14c27bbe41dfafb8e16f78e46c7f",
      "3303a81129b043e49a60e4a185572589",
      "a7833a8cad5541488493229d33358540",
      "0481906003314dcdb3d1f0fd68b92c75",
      "0b6d35bb9e014e658756429bb0dfa900",
      "8cc1f24400734eee89d8f5c222c98204",
      "b3914aff9b4d40c19a1dc2f0808c0826",
      "c9e91fff99d24225bf939842b4d7f20c",
      "8863679cf7ae4f2091cae03655029b09",
      "bd4d68d7877b4ace9c176e896170c014",
      "5a16b9ae989e477db93912743ca8b9a0",
      "7e97067d914f47faa7a1df370e58d6e6",
      "25e775310102447fb8ed6eaff121371e",
      "5b960328e2454cada5e028011d4cdc6a",
      "17b75930c67b4dff95a2583753e5b52b",
      "4a31705bf370469eb3bab27797d561bb",
      "de7ce7d561954ed29ebad64a0bebc6b0",
      "b6f668ea9784458a8c9fd318a34de255",
      "b606e64eca9245158c3745b4849ca799",
      "beabf1dda08345fc81af4747c1a87b3f",
      "538a62269b9346f98cdae2e9cced7985",
      "ff826545c0d44185bc9f5dfcea75e24c",
      "efd2b1d3d10f4c748cba241f69429b69",
      "ddca4d8fcaf241019d051c3201f9ed35",
      "91d268296c1041e2bda844aee123b74c",
      "df5285ccfc95465ebb93e0273086e073",
      "6614a1774170459a87e4acab978a1be1",
      "22557922bb694b04bae60c1f4d9fd6c2",
      "e44a2d2cdca84d1a9e964d01431aff28",
      "c92bf04166254e3bb990116ceeba3abe",
      "40f74695b42a4d9a8b0ce08f30315352",
      "267d0b473971489fb10844726c5971bf",
      "f601b771edc74b60b9fb5f3f6ef7697e",
      "883215e540a547b989f2fea1d4b05db1",
      "628cb1de93c44d56ab09a4441126129b",
      "05058af92e9e4b43a22a69f7b3bd68c6",
      "f37a749c1001467985f7746e1e1635bd"
     ]
    },
    "id": "IEJbmK0Ho_q6",
    "outputId": "f19b995a-4041-4c24-fcc4-6fbe0e238529"
   },
   "outputs": [],
   "source": [
    "#tokenization and padding using BERT Tokenizer\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class TweetDatasetBERT(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = tokenizer_bert.encode_plus(\n",
    "            self.texts[idx],\n",
    "            add_special_tokens=True,\n",
    "            max_length=100,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkaakA6tritA"
   },
   "source": [
    "As usual, the dataset is being split into 80% training set and 20% testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kc0rte_lpqz5"
   },
   "outputs": [],
   "source": [
    "#splitting the data\n",
    "X_train_bert, X_test_bert, y_train_bert, y_test_bert = train_test_split(\n",
    "    sampling_df_new['combined_features'].tolist(), sampling_df_new['Depressed'].tolist(), test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdc0mR9MrzLv"
   },
   "source": [
    "Basically converting the dataset to torch tensors and load them into dataloaders with batch size of 16, the reason behind this is similar to the reason mentioned above for biLSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpGzjEJlp34Q"
   },
   "outputs": [],
   "source": [
    "#DataLoader\n",
    "batch_size_bert = 16\n",
    "\n",
    "train_dataset_bert = TweetDatasetBERT(X_train_bert, y_train_bert)\n",
    "test_dataset_bert = TweetDatasetBERT(X_test_bert, y_test_bert)\n",
    "\n",
    "train_loader_bert = DataLoader(train_dataset_bert, batch_size=batch_size_bert, shuffle=True)\n",
    "test_loader_bert = DataLoader(test_dataset_bert, batch_size=batch_size_bert, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pdII3uqsjtm"
   },
   "source": [
    "Initializing the BERT transformer model and the rest is basically setting up and configuring the BERT model to utilize cuda if available else utilizing the cpu to train the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "2e25a2d8666e4a6a9ed2be0a3ee3f421",
      "2ad4604d27884cc08e8b59353156010a",
      "3cf39d5120974b97acfec20a2a162b34",
      "9d067697ec2846679d6b195f90ee47a7",
      "78117fdaa2f745dd9652a80c8c8640f9",
      "6498979911bf4b67a3b84fe6b3bd957e",
      "ec9c7d78a06a4740993a55eccf187374",
      "178e61f824374b108975555791bb4b16",
      "ee6a2bba92bf4a829786287d0a21d086",
      "55125e73ec844cc7aaa7e967b8beab54",
      "070641d9648644078419e0c6be6e06f6"
     ]
    },
    "id": "B_JW5Xe7qD-B",
    "outputId": "dcfe8e0f-c8ae-44ba-bf4d-49870d6ee675"
   },
   "outputs": [],
   "source": [
    "#defining the BERT model\n",
    "model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "device_bert = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_bert = model_bert.to(device_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVB6KrvTs74M"
   },
   "source": [
    "Setting up the optimizer for BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLL3UiLzqNKR"
   },
   "outputs": [],
   "source": [
    "#optimizer\n",
    "optimizer_bert = AdamW(model_bert.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "My74GG5ZtCAM"
   },
   "source": [
    "Defining BERT transformer training function which in a way very similar to biLSTM model training approach . Methods include :    \n",
    "\n",
    "Iterate the below steps using training set based on the number of epochs\n",
    "\n",
    "**Forward Pass**\n",
    "\n",
    "1.   reset the gradient to 0 via the optimizer.\n",
    "2.   predicting the output with BERT model by providing the input_ids and attention_mask.\n",
    "3.   initialize loss function which in this case would be a cross_entropy (mainly for classification problem) by providing the logits (termed as raw and unnormalized) for the prediction outcome and the actual target labels.\n",
    "\n",
    "**Backward propagation**\n",
    "\n",
    "4.  calling loss.backward() to compute loss and initiate backward propagation while keeping track of the gradients.\n",
    "5. optimizer will step forward to adjust and update the weights in the neural network based on the gradients captured in step 4 in order for the network to come to the right prediction or conclusion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HzITIacqUOH"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "#training function\n",
    "def train_model_bert(model_bert, train_loader_bert, optimizer_bert, device_bert, num_epochs=3):\n",
    "    model_bert.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in tqdm(train_loader_bert, desc=f'Epoch {epoch+1}'):\n",
    "            optimizer_bert.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device_bert)\n",
    "            attention_mask = batch['attention_mask'].to(device_bert)\n",
    "            labels = batch['labels'].to(device_bert)\n",
    "            # Handle out-of-vocabulary tokens\n",
    "            input_ids = torch.clamp(input_ids, max=tokenizer_bert.vocab_size - 1)\n",
    "            outputs = model_bert(input_ids, attention_mask=attention_mask)\n",
    "            # Calculate loss using outputs.logits and labels\n",
    "            loss = F.cross_entropy(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer_bert.step()\n",
    "        print(f'Epoch {epoch+1} completed. Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MS63_D62vQBK"
   },
   "source": [
    "evaluate_model_bert defines the function to evaluate the BERT model by first predicting the outcome, and extracting the accuracy, recall, F1-score from the predictions to render a bar chart to compare the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6Gt4lsPqg24"
   },
   "outputs": [],
   "source": [
    "#evaluation function\n",
    "def evaluate_model_bert(model_bert, test_loader_bert, device_bert):\n",
    "    model_bert.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader_bert:\n",
    "            input_ids = batch['input_ids'].to(device_bert)\n",
    "            attention_mask = batch['attention_mask'].to(device_bert)\n",
    "            labels = batch['labels'].to(device_bert)\n",
    "            outputs = model_bert(input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy_bert = accuracy_score(all_labels, all_preds)\n",
    "    recall_bert = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1_bert = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(\"BERT Classification Report:\")\n",
    "    print(f\"Accuracy: {accuracy_bert:.4f}\")\n",
    "    print(f\"Recall: {recall_bert:.4f}\")\n",
    "    print(f\"F1-Score: {f1_bert:.4f}\")\n",
    "\n",
    "    #visualization\n",
    "    metrics_bert = ['Accuracy', 'Recall', 'F1-Score']\n",
    "    scores_bert = [accuracy_bert, recall_bert, f1_bert]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(metrics_bert, scores_bert, color=['blue', 'green', 'red'])\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title('Performance Metrics for BERT Model')\n",
    "    for i, v in enumerate(scores_bert):\n",
    "        plt.text(i, v + 0.02, f\"{v:.2f}\", ha='center', fontweight='bold')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjkt-JS5v9C4"
   },
   "source": [
    "Training the model with BERT model, loader for training set, AdamW optimizer, device and num of epochs = 3 as input . Due to the resource intensive process of training a BERT model, and also limited resource in google collab platform, the number of epochs is limited to 3 as each run requires around 25 - 30 mins which take quite a lot of time. **(using CPU, unfortunately it crashes for GPU run though not sure why)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RqCtMkBzqt1g",
    "outputId": "7ee0c6d8-44c4-4987-af12-60dd09c6a9ee"
   },
   "outputs": [],
   "source": [
    "#training the model\n",
    "train_model_bert(model_bert, train_loader_bert, optimizer_bert, device_bert, num_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoD_796QxMiC"
   },
   "source": [
    "Finally, evaluating the BERT model . At least, from current observation, biLSTM actually perform better than BERT model, so BERT model probably have to be further fine-tuned but due to limited resources, BERT model cannot reach its potential since it is very resource intensive and not quite suitable for the task or use case at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "ja8Wn9EXN1Rx",
    "outputId": "9a2e9e5a-8bc1-488f-dddd-a6f1f89f74f9"
   },
   "outputs": [],
   "source": [
    "#evaluate model\n",
    "evaluate_model_bert(model_bert, test_loader_bert, device_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uInhbUen0KB0"
   },
   "source": [
    "## Prototype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AklTI-tvx-io"
   },
   "source": [
    "To summarize , ensemble stack model is chosen for this prototype as it perform far better than RF / LR / biLSTM / BERT in this specific scenario in terms of accuracy, pricision, recall and F1-score and also stability (referring to the consistency of performance even after  several iterations of trainings).\n",
    "\n",
    "This prototype will demonstrate the use of loaded ensemble stack model to predict 10 new chinese comments which is not in any of the dataset as it is randomly generated .\n",
    "\n",
    "Based on observations below, out of 10 comments, 2 were wrong so it work as intended though the accuracy can still be improved in several ways.\n",
    "\n",
    "** Note that in order for the prototype to work, please run section for data preprocessing and machine learning **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvmKpOBu0deK",
    "outputId": "a0233c6e-031d-44d3-eb03-5cdf49e381d0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "import thulac #https://github.com/thunlp/THULAC-Python\n",
    "import emoji\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords(tweet, chinese_tokenizer, stopwords):\n",
    "  tweet = chinese_tokenizer.cut(tweet, text=True)\n",
    "  features = tweet.split(\" \")\n",
    "  tweet = \" \".join([word for word in features if word not in stopwords])\n",
    "  return tweet\n",
    "\n",
    "def remove_noice(tweet):\n",
    "     tweet = re.sub(r\"[^\\u4e00-\\u9fff]\",\"\", tweet.strip())\n",
    "     return tweet.replace(\" \",\"\")\n",
    "\n",
    "def replace_emoji(tweet):\n",
    "  emoji_list = emoji.distinct_emoji_list(tweet)\n",
    "\n",
    "  def translate_emoji(emoji_icon):\n",
    "    first_token = next(emoji.analyze(emoji_icon))\n",
    "    return first_token.value.data['zh'] if first_token else \"\"\n",
    "\n",
    "  for emoji_icon in emoji_list:\n",
    "    tweet = tweet.replace(emoji_icon, translate_emoji(emoji_icon))\n",
    "  return tweet\n",
    "\n",
    "def get_sentiment_score(word, lexicon_df):\n",
    "    score_row = lexicon_df.loc[lexicon_df['context'] == word, 'depression_point']\n",
    "    if not score_row.empty:\n",
    "        return score_row.values[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_sentiment_score(comment, lexicon_df):\n",
    "    words = comment.split()\n",
    "    sentiment_score = sum([get_sentiment_score(word, lexicon_df) for word in words])\n",
    "    return sentiment_score\n",
    "\n",
    "\n",
    "def pipeline(comment, lexicon_df, tfidf_vectorizer, model, chinese_tokenizer, stopwords):\n",
    "    comment = replace_emoji(comment)\n",
    "    comment = remove_noice(comment)\n",
    "    comment = remove_stopwords(comment, chinese_tokenizer, stopwords)\n",
    "    sentiment_score = compute_sentiment_score(comment, lexicon_df)\n",
    "    tfidf_vector = tfidf_vectorizer.transform([comment]).toarray()\n",
    "    combined_features = np.hstack((tfidf_vector, np.array([[sentiment_score]])))\n",
    "    depression_state = model.predict(combined_features)\n",
    "\n",
    "    return depression_state\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    lexicon_df = pd.read_csv(\"/content/DepressionDetectionDataset_Chinese/depressionLexiconNew.csv\", encoding=\"utf-8\")\n",
    "    depressed_lexicon_keywords = lexicon_df[lexicon_df['depression_point'] == 1]['context'].tolist()\n",
    "    healthy_lexicon_keywords = lexicon_df[lexicon_df['depression_point'] == 0]['context'].tolist()\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    stopwords = stopwords.words( 'chinese' )\n",
    "    stopwords = [word for word in stopwords if word not in depressed_lexicon_keywords]\n",
    "    stopwords = [word for word in stopwords if word not in healthy_lexicon_keywords]\n",
    "\n",
    "    chinese_tokenizer = thulac.thulac(seg_only=True)\n",
    "\n",
    "    tfidf_vectorizer = load(os.path.join(directory_name, \"tfidf_vectorizer.pkl\"))\n",
    "    ensemble_model = load(os.path.join(directory_name, \"stacking_classifier_model.pkl\"))\n",
    "\n",
    "    comments = [\n",
    "        \"最近真的好累，感觉什么都提不起兴趣…😞 生活好像失去了颜色。\",\n",
    "        \"29岁了，怎么感觉人生越来越迷茫？每天都在对抗自己，好累…😔\",\n",
    "        \"每天醒来都觉得心好沉重，真的不知道该怎么办…😢\",\n",
    "        \"看着别人都在前进，而我却原地踏步，甚至在退步…😭 我到底怎么了？\",\n",
    "        \"每天都在假装开心，但其实内心已经快撑不下去了…😣 谁能懂这种感觉？\",\n",
    "        \"今天的天气真好，阳光明媚，心情也特别好！🌞 要好好享受这美好的一天！\",\n",
    "        \"坚持锻炼的第30天，感觉身体越来越健康了！💪 继续加油！\",\n",
    "        \"刚刚学会了一道新菜，味道超级棒！🍲 生活中的小确幸真是美好！\",\n",
    "        \"和朋友们一起度过了一个美好的周末，真是充满了能量！😊 生活需要这样的小快乐！\",\n",
    "        \"最近工作特别顺利，感觉自己的努力都得到了回报！💼 幸福感爆棚！\"\n",
    "    ]\n",
    "\n",
    "    for comment in comments:\n",
    "        depression_state = pipeline(comment, lexicon_df, tfidf_vectorizer, ensemble_model, chinese_tokenizer, stopwords)\n",
    "        print(\"Comment : {} , Predicted Depression State: [ {} ] \".format(comment,\"depressed\" if depression_state[0] == 1 else \"healthy\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smXEtuVIzgGJ"
   },
   "source": [
    "# Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnfUsQkGzebG"
   },
   "source": [
    "In our opinion, the ensemble stack model probably did well but there might be more ways to further improve further. A few ways that we could think of include :     \n",
    "\n",
    "\n",
    "\n",
    "*   Introducing a voting mechanism to include prediction from different models. In this case these models would be stack ensemble model, biLSTM model, and probably google gemini LLM API or any other strong model. So , basically it would work similarly to a decision making process by the board in a company or election ballot. Majority wins the vote and from the average , compute the confidence level, thus anythin above confidence level of 0.5 and above will considered as depressed comments / tweets while anything below 0.5 will be considered as healthy comments / tweets.\n",
    "\n",
    "\n",
    "*  Improvise further on feature engineering.\n",
    "\n",
    "*  Also include more resources in order to include enough data but not over-provide to reduce risk of overfitting due to unnecessary feature learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prcrHs4DQYYl"
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIxOzfxZQhjT"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "*   https://github.com/thunlp/THULAC-Python\n",
    "*   https://github.com/ethan-nicholas-tsai/SWDD/tree/main\n",
    "*   https://zenpulsar.com/news/tpost/5h9c8xbti1-evolution-of-sentiment-analysis-from-bas\n",
    "*   https://towardsdatascience.com/stacking-classifiers-for-higher-predictive-performance-566f963e4840\n",
    "*   https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier\n",
    "*   https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier\n",
    "*   https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "*   https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "* https://github.com/abuBakarSiddiqurRahman/DepressionEmo/tree/main\n",
    "* https://github.com/raju-shrestha/Sentiment-Analysis-of-Twitter-Data-Using-Logistic-Regression\n",
    "* https://www.kaggle.com/code/kevinmorgado/twitter-sentiment-analysis-logistic-regression\n",
    "* https://www.kaggle.com/code/langkilde/linear-svm-classification-of-sentiment-in-tweets\n",
    "* https://github.com/soham2707/Twitter-Sentiment-Analysis-\n",
    "* https://github.com/andikarachman/RNN-Twitter-Sentiment-Analysis\n",
    "* https://www.kaggle.com/code/ludovicocuoghi/twitter-sentiment-analysis-with-bert-vs-roberta\n",
    "* https://github.com/baotramduong/Twitter-Sentiment-Analysis-with-Deep-Learning-using-BERT\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
