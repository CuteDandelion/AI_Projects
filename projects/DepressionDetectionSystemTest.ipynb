{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQPeZB-a0otw"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZiAm59hcB6-"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrGluC0Tvtya"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYwmURgoZKKf",
    "outputId": "52284ae8-938c-437a-874b-a69e004e3647"
   },
   "outputs": [],
   "source": [
    "pip install emoji google_trans_new thulac gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "l_RPI_2A_Qi1",
    "outputId": "c9695ab0-4a00-48a8-a8c9-bc5c30babe24"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "control_file = 'control_comments_male.txt'\n",
    "depressed_file = 'depressed_comments_male.txt'\n",
    "\n",
    "with open(depressed_file, \"r\", encoding=\"utf-8\") as d:\n",
    "    d_lines = d.readlines()\n",
    "    d_lines = [x.replace(\"\\n\",\"\") for x in d_lines]\n",
    "\n",
    "with open(control_file, \"r\", encoding=\"utf-8\") as c:\n",
    "    c_lines = c.readlines()\n",
    "    c_lines = [x.replace(\"\\n\",\"\") for x in c_lines]\n",
    "\n",
    "depressed = pd.DataFrame(d_lines,columns=[\"Tweet\"])\n",
    "depressed[\"Depressed\"] = 1\n",
    "control = pd.DataFrame(c_lines,columns=[\"Tweet\"])\n",
    "control[\"Depressed\"] = 0\n",
    "\n",
    "df = pd.concat([depressed, control], ignore_index=True)\n",
    "df = df[df['Tweet'] != '']\n",
    "df = df[df['Tweet'] != '转发微博']\n",
    "df = df.sample(frac = 1)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "aGEVjU-L_ZjD",
    "outputId": "363ec7ba-792f-4e33-ed56-5066b5df318b"
   },
   "outputs": [],
   "source": [
    "print(\"Dataset size:\",df.shape)\n",
    "df.groupby(\"Depressed\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c75MJEq-iook",
    "outputId": "0ce59da7-d138-4cbe-a094-daae55f5c19c"
   },
   "outputs": [],
   "source": [
    "from google_trans_new import google_translator as Translator\n",
    "from joblib import Parallel, delayed\n",
    "import thulac #https://github.com/thunlp/THULAC-Python\n",
    "\n",
    "translator = Translator()\n",
    "chinese_tokenizer = thulac.thulac(seg_only=True)\n",
    "\n",
    "def remove_noice(tweet):\n",
    "     tweet = re.sub(r\"[^\\u4e00-\\u9fff]\",\"\", tweet.strip())\n",
    "     return tweet.replace(\" \",\"\")\n",
    "\n",
    "def replace_emoji(tweet):\n",
    "  emoji_list = emoji.distinct_emoji_list(tweet)\n",
    "\n",
    "  def translate_emoji(emoji_icon):\n",
    "    first_token = next(emoji.analyze(emoji_icon))\n",
    "    return first_token.value.data['zh'] if first_token else \"\"\n",
    "\n",
    "  for emoji_icon in emoji_list:\n",
    "    tweet = tweet.replace(emoji_icon, translate_emoji(emoji_icon))\n",
    "  return tweet\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"emojilessTweet\"] = df[\"Tweet\"].apply(lambda x: replace_emoji(x))\n",
    "df[\"noiselessTweet\"] = df[\"emojilessTweet\"].apply(lambda x: remove_noice(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TMWZTcStqCgR",
    "outputId": "068a0601-4beb-43da-d896-677caee5f0e2"
   },
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JMkev3V_Jmwe",
    "outputId": "92bbf25b-0476-4d2b-dc51-b0773b7facdb"
   },
   "outputs": [],
   "source": [
    "# Due to the dataset is large, a lexicon is used to filter the dataframe to extract rows that are highly relevant.\n",
    "\n",
    "lexicon_df = pd.read_csv(\"depressionLexiconNew.csv\", encoding=\"utf-8\")\n",
    "print(lexicon_df[\"context\"][lexicon_df[\"depression_point\"]==1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Lfc1Zoj4m_gU",
    "outputId": "21492dbc-a216-4db9-a139-f5f120bf9817"
   },
   "outputs": [],
   "source": [
    "depressed_lexicon_keywords = lexicon_df[\"context\"][lexicon_df[\"depression_point\"]==1].tolist()\n",
    "pattern = re.compile('|'.join(depressed_lexicon_keywords))\n",
    "\n",
    "filtered_depressed_df = df[df['Depressed'] == 1]\n",
    "filtered_depressed_df = filtered_depressed_df[filtered_depressed_df[\"noiselessTweet\"].astype(str).apply(lambda x: pattern.search(x) is not None)]\n",
    "filtered_depressed_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CtDFxmytw7bN",
    "outputId": "15be910f-7701-47c9-e0da-7ad70930faff"
   },
   "outputs": [],
   "source": [
    "healthy_lexicon_keywords = lexicon_df[\"context\"][lexicon_df[\"depression_point\"]==0].tolist()\n",
    "pattern = re.compile('|'.join(healthy_lexicon_keywords))\n",
    "\n",
    "filtered_healthy_df = df[df['Depressed'] == 0]\n",
    "filtered_healthy_df = filtered_healthy_df[filtered_healthy_df[\"noiselessTweet\"].astype(str).apply(lambda x: pattern.search(x) is not None)]\n",
    "filtered_healthy_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "kx4tkS18xBOT",
    "outputId": "b8100048-fb47-4899-e329-6bceb8d61ff3"
   },
   "outputs": [],
   "source": [
    "### Sample dataframe after filtering with lexicon\n",
    "sampling_df = pd.concat([filtered_depressed_df, filtered_healthy_df], ignore_index=True)\n",
    "sampling_df = sampling_df.sample(frac = 1)\n",
    "sampling_df.reset_index(drop=True, inplace=True)\n",
    "sampling_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UtSYdIhKIQ5P",
    "outputId": "f2fca1a0-1e68-48ff-aae9-ba197ae5bb65"
   },
   "outputs": [],
   "source": [
    "sampling_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "kXGFgLVSx0WR",
    "outputId": "0b065a56-2eb7-4eaa-d45b-26d9ea7f8350"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words( 'chinese' )\n",
    "stopwords = [word for word in stopwords if word not in depressed_lexicon_keywords]\n",
    "stopwords = [word for word in stopwords if word not in healthy_lexicon_keywords]\n",
    "\n",
    "def remove_stopwords(tweet):\n",
    "  tweet = chinese_tokenizer.cut(tweet, text=True)\n",
    "  features = tweet.split(\" \")\n",
    "  tweet = \"\".join([word for word in features if word not in stopwords])\n",
    "  return tweet\n",
    "\n",
    "sampling_df[\"cleanTweet\"] = sampling_df[\"noiselessTweet\"].apply(lambda x: remove_stopwords(x))\n",
    "sampling_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8PRxMhRfIPXT",
    "outputId": "57d435ea-0a60-4280-d2c3-ee75ad3f1ee0"
   },
   "outputs": [],
   "source": [
    "sampling_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIuB5Tg4jaog"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "TweetList = df[\"noiselessTweet\"].tolist()\n",
    "TweetCorpus = []\n",
    "for tweet in TweetList:\n",
    "  tweet = chinese_tokenizer.cut(tweet, text=True)\n",
    "  TweetCorpus.append(tweet.split(\" \"))\n",
    "\n",
    "model = gensim.models.Word2Vec(TweetCorpus, window=5, min_count=1, workers=4)\n",
    "model.save(\"tweets.model\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
